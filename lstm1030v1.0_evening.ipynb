{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindtorch.torch as torch\n",
    "import pandas as pd\n",
    "import mindtorch.torch.nn as nn\n",
    "import mindtorch.torch.optim as optim\n",
    "from mindtorch.torch.utils.data import DataLoader, TensorDataset\n",
    "import mindtorch.torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "#import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集前5条数据:\n",
      "     user_id        date  steps  exercise_time  avg_heart_rate  \\\n",
      "999        1  2022-02-04  11470             22              68   \n",
      "998        1  2022-02-05   9290             27              69   \n",
      "997        1  2022-02-06   7218             59              85   \n",
      "996        1  2022-02-07   9572             65              81   \n",
      "995        1  2022-02-08   7615             36              66   \n",
      "\n",
      "     max_heart_rate  sleep_duration  fatigue_level  relaxation_training  \\\n",
      "999             108        5.449322       6.107494                    0   \n",
      "998             104        5.528365       5.405949                    0   \n",
      "997             111        6.309658       2.857095                    0   \n",
      "996             124        8.606211       3.488209                    0   \n",
      "995              93        7.023586       6.106323                    0   \n",
      "\n",
      "     height  weight  age  \n",
      "999     175      70   30  \n",
      "998     175      70   30  \n",
      "997     175      70   30  \n",
      "996     175      70   30  \n",
      "995     175      70   30  \n",
      "\n",
      "验证集前5条数据:\n",
      "     user_id        date  steps  exercise_time  avg_heart_rate  \\\n",
      "199        1  2024-04-14  11967             52              79   \n",
      "198        1  2024-04-15   4056             60              84   \n",
      "197        1  2024-04-16   4194             71              89   \n",
      "196        1  2024-04-17   6326             40              76   \n",
      "195        1  2024-04-18   9981             25              64   \n",
      "\n",
      "     max_heart_rate  sleep_duration  fatigue_level  relaxation_training  \\\n",
      "199             117        6.509707       1.435945                    0   \n",
      "198             133        8.810334       4.032834                    0   \n",
      "197             130        6.431391       8.260945                    1   \n",
      "196             103        7.708507       3.545503                    0   \n",
      "195              99        5.324609       5.972937                    0   \n",
      "\n",
      "     height  weight  age  \n",
      "199     175      70   30  \n",
      "198     175      70   30  \n",
      "197     175      70   30  \n",
      "196     175      70   30  \n",
      "195     175      70   30  \n"
     ]
    }
   ],
   "source": [
    "# 读取已有的 CSV 文件\n",
    "input_file = 'user_health_data03.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# 按每个用户划分训练集和验证集\n",
    "def split_data_by_user(df, train_ratio=0.8):\n",
    "    user_ids = df['user_id'].unique()\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        user_data = df[df['user_id'] == user_id].sort_values(by='date')\n",
    "        train_size = int(len(user_data) * train_ratio)\n",
    "        train_data.append(user_data.iloc[:train_size])\n",
    "        val_data.append(user_data.iloc[train_size:])\n",
    "    \n",
    "    train_df = pd.concat(train_data)\n",
    "    val_df = pd.concat(val_data)\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "# 将数据按用户划分为训练集和验证集\n",
    "train_df, val_df = split_data_by_user(df)\n",
    "\n",
    "# 保存数据到 CSV 文件\n",
    "train_df.to_csv('train_data_with_noise.csv', index=False)\n",
    "val_df.to_csv('val_data_with_noise.csv', index=False)\n",
    "\n",
    "# 查看部分生成的数据\n",
    "print(\"训练集前5条数据:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n验证集前5条数据:\")\n",
    "print(val_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成滑动窗口数据\n",
    "def create_sliding_window_data(df, window_size=14):\n",
    "    X, y = [], []\n",
    "    \n",
    "    # 对每个用户单独处理\n",
    "    for user_id in df['user_id'].unique():\n",
    "        user_data = df[df['user_id'] == user_id].sort_values(by='date')\n",
    "        features = user_data[['steps', 'exercise_time', 'avg_heart_rate', 'max_heart_rate', \n",
    "                              'sleep_duration', 'fatigue_level', \n",
    "                              'height', 'weight', 'age']]\n",
    "        target = user_data['relaxation_training']  # 目标是 relaxation_training\n",
    "        \n",
    "        # 使用滑动窗口生成数据\n",
    "        for i in range(len(user_data) - window_size):\n",
    "            X.append(features.iloc[i:i+window_size].values)  # 14天的输入数据\n",
    "            y.append(target.iloc[i+window_size])  # 第15天的目标值\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 X 维度: (2391, 3, 9)\n",
      "训练集 y 维度: (2391,)\n",
      "验证集 X 维度: (594, 2, 9)\n",
      "验证集 y 维度: (594,)\n"
     ]
    }
   ],
   "source": [
    "# 加载训练数据\n",
    "df_train = pd.read_csv('train_data_with_noise.csv')  # 假设你的数据在这个文件中\n",
    "X_train, y_train = create_sliding_window_data(df_train, window_size=3)\n",
    "\n",
    "# 加载验证数据\n",
    "df_val = pd.read_csv('val_data_with_noise.csv')  # 假设验证数据在这个文件中\n",
    "X_val, y_val = create_sliding_window_data(df_val, window_size=2)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "num_features = X_train.shape[2]  # 特征数量\n",
    "\n",
    "# 标准化训练数据\n",
    "X_train_reshaped = X_train.reshape(-1, num_features)  # 调整为二维\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)  # 标准化\n",
    "X_train_scaled = X_train_scaled.reshape(X_train.shape)  # 调整回三维\n",
    "\n",
    "# 标准化验证数据\n",
    "X_val_reshaped = X_val.reshape(-1, num_features)  # 调整为二维\n",
    "X_val_scaled = scaler.transform(X_val_reshaped)  # 使用训练集的 scaler 进行标准化\n",
    "X_val_scaled = X_val_scaled.reshape(X_val.shape)  # 调整回三维\n",
    "\n",
    "# 打印数据维度\n",
    "print(\"训练集 X 维度:\", X_train_scaled.shape)  # (num_samples, window_size, num_features)\n",
    "print(\"训练集 y 维度:\", y_train.shape)  # (num_samples,)\n",
    "print(\"验证集 X 维度:\", X_val_scaled.shape)\n",
    "print(\"验证集 y 维度:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为 PyTorch 张量\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y_tensor = torch.tensor(train_y, dtype=torch.float32)\n",
    "val_X_tensor = torch.tensor(val_X, dtype=torch.float32)\n",
    "val_y_tensor = torch.tensor(val_y, dtype=torch.float32)\n",
    "\n",
    "# 创建训练集和验证集的 TensorDataset\n",
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "val_dataset = TensorDataset(val_X_tensor, val_y_tensor)\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 64  # 批大小\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, num_heads, dropout_rate=0.5):\n",
    "        super(AttentionLSTMModel, self).__init__()\n",
    "        \n",
    "        # 双层 LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size1, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_size1, hidden_size=hidden_size2, num_layers=1, batch_first=True)\n",
    "        \n",
    "        # 多头注意力层\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size2, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # 全连接层部分\n",
    "        self.fc1 = nn.Linear(hidden_size2, 8)  # 第一个全连接层，将 32 维压缩到 16 维\n",
    "        self.relu = nn.ReLU()                   # 激活函数\n",
    "        self.fc2 = nn.Linear(8, output_size)   # 第二个全连接层，将 16 维压缩到 1 维\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM 层\n",
    "        lstm_out, _ = self.lstm1(x)\n",
    "        lstm_out, _ = self.lstm2(lstm_out)\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # 多头注意力层\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # 取最后一个时间步的注意力输出\n",
    "        attn_output = attn_output[:, -1, :]\n",
    "        \n",
    "        # 全连接层和激活函数\n",
    "        out = self.fc1(attn_output)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# 定义超参数\n",
    "input_size = 9  # 输入特征数\n",
    "hidden_size1 = 32 # 第一层 LSTM 隐藏层大小\n",
    "hidden_size2 = 16  # 第二层 LSTM 隐藏层大小\n",
    "output_size = 1  # 输出为标量，用于回归\n",
    "num_heads = 2  # 多头注意力头数\n",
    "learning_rate = 0.02  # 学习率\n",
    "num_epochs = 100  # 训练的轮数\n",
    "batch_size = 64  # 批大小\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLSTMModel(\n",
      "  (lstm1): LSTM(9, 32, batch_first=True)\n",
      "  (lstm2): LSTM(32, 16, batch_first=True)\n",
      "  (attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [1/100], Loss: 0.1982, Learning Rate: 0.020000\n",
      "Epoch [2/100], Loss: 0.1878, Learning Rate: 0.020000\n",
      "Epoch [3/100], Loss: 0.1803, Learning Rate: 0.020000\n",
      "Epoch [4/100], Loss: 0.1760, Learning Rate: 0.020000\n",
      "Epoch [5/100], Loss: 0.1731, Learning Rate: 0.020000\n",
      "Epoch [6/100], Loss: 0.1743, Learning Rate: 0.020000\n",
      "Epoch [7/100], Loss: 0.1635, Learning Rate: 0.020000\n",
      "Epoch [8/100], Loss: 0.1582, Learning Rate: 0.020000\n",
      "Epoch [9/100], Loss: 0.1488, Learning Rate: 0.020000\n",
      "Epoch [10/100], Loss: 0.1435, Learning Rate: 0.002000\n",
      "Epoch [11/100], Loss: 0.1357, Learning Rate: 0.002000\n",
      "Epoch [12/100], Loss: 0.1225, Learning Rate: 0.002000\n",
      "Epoch [13/100], Loss: 0.1171, Learning Rate: 0.002000\n",
      "Epoch [14/100], Loss: 0.1127, Learning Rate: 0.002000\n",
      "Epoch [15/100], Loss: 0.1115, Learning Rate: 0.002000\n",
      "Epoch [16/100], Loss: 0.1098, Learning Rate: 0.002000\n",
      "Epoch [17/100], Loss: 0.1070, Learning Rate: 0.002000\n",
      "Epoch [18/100], Loss: 0.1059, Learning Rate: 0.002000\n",
      "Epoch [19/100], Loss: 0.1025, Learning Rate: 0.002000\n",
      "Epoch [20/100], Loss: 0.0985, Learning Rate: 0.000200\n",
      "Epoch [21/100], Loss: 0.0945, Learning Rate: 0.000200\n",
      "Epoch [22/100], Loss: 0.0931, Learning Rate: 0.000200\n",
      "Epoch [23/100], Loss: 0.0923, Learning Rate: 0.000200\n",
      "Epoch [24/100], Loss: 0.0919, Learning Rate: 0.000200\n",
      "Epoch [25/100], Loss: 0.0876, Learning Rate: 0.000200\n",
      "Epoch [26/100], Loss: 0.0917, Learning Rate: 0.000200\n",
      "Epoch [27/100], Loss: 0.0911, Learning Rate: 0.000200\n",
      "Epoch [28/100], Loss: 0.0919, Learning Rate: 0.000200\n",
      "Epoch [29/100], Loss: 0.0906, Learning Rate: 0.000200\n",
      "Epoch [30/100], Loss: 0.0904, Learning Rate: 0.000020\n",
      "Epoch [31/100], Loss: 0.0877, Learning Rate: 0.000020\n",
      "Epoch [32/100], Loss: 0.0899, Learning Rate: 0.000020\n",
      "Epoch [33/100], Loss: 0.0874, Learning Rate: 0.000020\n",
      "Epoch [34/100], Loss: 0.0906, Learning Rate: 0.000020\n",
      "Epoch [35/100], Loss: 0.0904, Learning Rate: 0.000020\n",
      "Epoch [36/100], Loss: 0.0914, Learning Rate: 0.000020\n",
      "Epoch [37/100], Loss: 0.0880, Learning Rate: 0.000020\n",
      "Epoch [38/100], Loss: 0.0882, Learning Rate: 0.000020\n",
      "Epoch [39/100], Loss: 0.0903, Learning Rate: 0.000020\n",
      "Epoch [40/100], Loss: 0.0888, Learning Rate: 0.000002\n",
      "Epoch [41/100], Loss: 0.0894, Learning Rate: 0.000002\n",
      "Epoch [42/100], Loss: 0.0873, Learning Rate: 0.000002\n",
      "Epoch [43/100], Loss: 0.0893, Learning Rate: 0.000002\n",
      "Epoch [44/100], Loss: 0.0875, Learning Rate: 0.000002\n",
      "Epoch [45/100], Loss: 0.0906, Learning Rate: 0.000002\n",
      "Epoch [46/100], Loss: 0.0882, Learning Rate: 0.000002\n",
      "Epoch [47/100], Loss: 0.0897, Learning Rate: 0.000002\n",
      "Epoch [48/100], Loss: 0.0899, Learning Rate: 0.000002\n",
      "Epoch [49/100], Loss: 0.0904, Learning Rate: 0.000002\n",
      "Epoch [50/100], Loss: 0.0870, Learning Rate: 0.000000\n",
      "Epoch [51/100], Loss: 0.0889, Learning Rate: 0.000000\n",
      "Epoch [52/100], Loss: 0.0890, Learning Rate: 0.000000\n",
      "Epoch [53/100], Loss: 0.0892, Learning Rate: 0.000000\n",
      "Epoch [54/100], Loss: 0.0882, Learning Rate: 0.000000\n",
      "Epoch [55/100], Loss: 0.0882, Learning Rate: 0.000000\n",
      "Epoch [56/100], Loss: 0.0891, Learning Rate: 0.000000\n",
      "Epoch [57/100], Loss: 0.0901, Learning Rate: 0.000000\n",
      "Epoch [58/100], Loss: 0.0878, Learning Rate: 0.000000\n",
      "Epoch [59/100], Loss: 0.0877, Learning Rate: 0.000000\n",
      "Epoch [60/100], Loss: 0.0895, Learning Rate: 0.000000\n",
      "Epoch [61/100], Loss: 0.0903, Learning Rate: 0.000000\n",
      "Epoch [62/100], Loss: 0.0897, Learning Rate: 0.000000\n",
      "Epoch [63/100], Loss: 0.0887, Learning Rate: 0.000000\n",
      "Epoch [64/100], Loss: 0.0905, Learning Rate: 0.000000\n",
      "Epoch [65/100], Loss: 0.0886, Learning Rate: 0.000000\n",
      "Epoch [66/100], Loss: 0.0901, Learning Rate: 0.000000\n",
      "Epoch [67/100], Loss: 0.0886, Learning Rate: 0.000000\n",
      "Epoch [68/100], Loss: 0.0886, Learning Rate: 0.000000\n",
      "Epoch [69/100], Loss: 0.0895, Learning Rate: 0.000000\n",
      "Epoch [70/100], Loss: 0.0890, Learning Rate: 0.000000\n",
      "Epoch [71/100], Loss: 0.0896, Learning Rate: 0.000000\n",
      "Epoch [72/100], Loss: 0.0905, Learning Rate: 0.000000\n",
      "Epoch [73/100], Loss: 0.0888, Learning Rate: 0.000000\n",
      "Epoch [74/100], Loss: 0.0878, Learning Rate: 0.000000\n",
      "Epoch [75/100], Loss: 0.0889, Learning Rate: 0.000000\n",
      "Epoch [76/100], Loss: 0.0874, Learning Rate: 0.000000\n",
      "Epoch [77/100], Loss: 0.0899, Learning Rate: 0.000000\n",
      "Epoch [78/100], Loss: 0.0882, Learning Rate: 0.000000\n",
      "Epoch [79/100], Loss: 0.0880, Learning Rate: 0.000000\n",
      "Epoch [80/100], Loss: 0.0907, Learning Rate: 0.000000\n",
      "Epoch [81/100], Loss: 0.0890, Learning Rate: 0.000000\n",
      "Epoch [82/100], Loss: 0.0867, Learning Rate: 0.000000\n",
      "Epoch [83/100], Loss: 0.0883, Learning Rate: 0.000000\n",
      "Epoch [84/100], Loss: 0.0886, Learning Rate: 0.000000\n",
      "Epoch [85/100], Loss: 0.0887, Learning Rate: 0.000000\n",
      "Epoch [86/100], Loss: 0.0866, Learning Rate: 0.000000\n",
      "Epoch [87/100], Loss: 0.0899, Learning Rate: 0.000000\n",
      "Epoch [88/100], Loss: 0.0872, Learning Rate: 0.000000\n",
      "Epoch [89/100], Loss: 0.0892, Learning Rate: 0.000000\n",
      "Epoch [90/100], Loss: 0.0914, Learning Rate: 0.000000\n",
      "Epoch [91/100], Loss: 0.0888, Learning Rate: 0.000000\n",
      "Epoch [92/100], Loss: 0.0890, Learning Rate: 0.000000\n",
      "Epoch [93/100], Loss: 0.0878, Learning Rate: 0.000000\n",
      "Epoch [94/100], Loss: 0.0875, Learning Rate: 0.000000\n",
      "Epoch [95/100], Loss: 0.0893, Learning Rate: 0.000000\n",
      "Epoch [96/100], Loss: 0.0877, Learning Rate: 0.000000\n",
      "Epoch [97/100], Loss: 0.0893, Learning Rate: 0.000000\n",
      "Epoch [98/100], Loss: 0.0878, Learning Rate: 0.000000\n",
      "Epoch [99/100], Loss: 0.0889, Learning Rate: 0.000000\n",
      "Epoch [100/100], Loss: 0.0911, Learning Rate: 0.000000\n",
      "Validation Loss: 0.1898\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = AttentionLSTMModel(input_size=input_size, hidden_size1=hidden_size1, hidden_size2=hidden_size2, output_size=output_size, num_heads=num_heads)\n",
    "print(model)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 使用均方误差损失\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2 正则化\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # 每10个epoch将学习率乘以0.1\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    #print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 验证模型\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), targets)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "print(f\"Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "torch.save(model, 'lstm_model_full.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
